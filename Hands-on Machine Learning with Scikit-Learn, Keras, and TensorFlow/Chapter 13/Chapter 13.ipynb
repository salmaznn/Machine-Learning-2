{"cells":[{"cell_type":"markdown","metadata":{"id":"a7934901"},"source":["# **Chapter 13: Efficient Data Pipelines Guide**"]},{"cell_type":"markdown","metadata":{"id":"5e916a4a"},"source":["## 1. Introduction to TensorFlow Data API\n","\n","The Data API provides tools to efficiently load, preprocess, and feed data to your models. Key benefits include:\n","- Handling datasets too large to fit in memory\n","- Optimized performance with prefetching and parallel processing\n","- Seamless integration with tf.keras\n","\n","### Core Concepts:\n","- `tf.data.Dataset`: Represents a sequence of data items\n","- Transformations: Methods like `map()`, `batch()`, `shuffle()`\n","- Iteration: Process datasets in batches during training"]},{"cell_type":"markdown","metadata":{"id":"2b08001b"},"source":["## 2. Creating Datasets\n","\n","### 2.1 From In-Memory Data\n","Create datasets from Python structures:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"d67c5059","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750434690874,"user_tz":-420,"elapsed":18441,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"16e3e31e-d3f6-4a13-f6cf-dfe8dc4a7eb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.44015409, 0.60324627, 0.74666876, 0.16445556, 0.65270905])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>)\n","(<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.38414335, 0.96022546, 0.25301543, 0.37172385, 0.75361583])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)\n","(<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.37657098, 0.0852118 , 0.6453096 , 0.83176716, 0.80311083])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)\n"]}],"source":["# Mengimpor pustaka TensorFlow dan modul lain yang dibutuhkan\n","import tensorflow as tf\n","# Mengimpor pustaka TensorFlow dan modul lain yang dibutuhkan\n","import numpy as np\n","\n","# From numpy arrays\n","data = np.array([1, 2, 3, 4, 5])\n","# Membuat pipeline data menggunakan tf.data API\n","dataset = tf.data.Dataset.from_tensor_slices(data)\n","\n","# From multiple arrays (features and labels)\n","features = np.random.rand(100, 5)\n","labels = np.random.randint(0, 2, size=(100, 1))\n","# Membuat pipeline data menggunakan tf.data API\n","dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n","\n","# Inspect the dataset\n","for element in dataset.take(3):\n","    print(element)"]},{"cell_type":"markdown","metadata":{"id":"a728ff39"},"source":["### 2.2 From Text Files\n","Read data from text files line by line:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"feb0e5dc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750434691378,"user_tz":-420,"elapsed":507,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"5dca2068-8bc1-4387-f5c4-99c4bcbf9514"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'Sample line 1 in file 2'\n","b'Sample line 1 in file 1'\n","b'Sample line 1 in file 0'\n","b'Sample line 2 in file 2'\n"]}],"source":["# Create sample text files\n","# Mengimpor pustaka TensorFlow dan modul lain yang dibutuhkan\n","import os\n","os.makedirs(\"data\", exist_ok=True)\n","for i in range(3):\n","    with open(f\"data/file_{i}.txt\", \"w\") as f:\n","        f.write(f\"Sample line 1 in file {i}\\n\")\n","        f.write(f\"Sample line 2 in file {i}\\n\")\n","\n","# Create dataset from text files\n","file_pattern = \"data/file_*.txt\"\n","# Membuat pipeline data menggunakan tf.data API\n","dataset = tf.data.Dataset.list_files(file_pattern)\n","\n","def read_file(file_path):\n","# Membuat pipeline data menggunakan tf.data API\n","    return tf.data.TextLineDataset(file_path)\n","\n","dataset = dataset.interleave(\n","    read_file,\n","    cycle_length=3,\n","# Membuat pipeline data menggunakan tf.data API\n","    num_parallel_calls=tf.data.AUTOTUNE\n",")\n","\n","for line in dataset.take(4):\n","    print(line.numpy())"]},{"cell_type":"markdown","metadata":{"id":"bf672a3c"},"source":["## 3. Data Preprocessing\n","\n","### 3.1 Using Dataset.map()\n","Apply transformations to each element:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9f7c0e27","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750434764956,"user_tz":-420,"elapsed":163,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"8fed86a9-197d-409b-9600-d0813b9d0168"},"outputs":[{"output_type":"stream","name":"stdout","text":["-0.1667198\n","0.97596556\n","3.9963362\n","9.033666\n","16.04558\n"]}],"source":["import tensorflow as tf\n","\n","# Membuat dataset angka 0â€“9\n","dataset = tf.data.Dataset.range(10)\n","\n","# Fungsi preprocessing\n","def square(x):\n","    return x ** 2\n","\n","def add_noise(x):\n","    x = tf.cast(x, tf.float32)  # Konversi ke float32 agar kompatibel\n","    return x + tf.random.normal(shape=(), mean=0.0, stddev=0.1)\n","\n","# Apply transformasi\n","dataset = dataset.map(square).map(add_noise)\n","\n","# Tampilkan contoh\n","for element in dataset.take(5):\n","    print(element.numpy())\n"]},{"cell_type":"markdown","metadata":{"id":"cf69f04f"},"source":["### 3.2 Preprocessing Images\n","Complete pipeline for image data:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"91ec0415","executionInfo":{"status":"ok","timestamp":1750434767433,"user_tz":-420,"elapsed":55,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["def preprocess_image(image_path, label):\n","    # Read and decode image\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","\n","    # Resize and normalize\n","    image = tf.image.resize(image, [224, 224])\n","    image = image / 255.0  # Normalize to [0,1]\n","\n","    return image, label\n","\n","# Example usage with dummy data\n","image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\"]  # Replace with actual paths\n","labels = [0, 1]\n","\n","# Membuat pipeline data menggunakan tf.data API\n","dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","# Membuat pipeline data menggunakan tf.data API\n","dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"e9a44495"},"source":["## 4. Performance Optimization\n","\n","### 4.1 Essential Optimization Techniques\n","\n","| Technique          | Method                 | Benefit                          |\n","|--------------------|------------------------|----------------------------------|\n","| Prefetching        | `.prefetch(buffer_size)` | Overlaps data prep and training  |\n","| Parallel Processing| `.map(..., num_parallel_calls)` | Uses multiple CPU cores       |\n","| Caching            | `.cache()`             | Avoids reprocessing              |\n","| Batching           | `.batch(batch_size)`   | Processes data in batches        |\n","\n","### 4.2 Complete Optimized Pipeline"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"e5d9a839","executionInfo":{"status":"ok","timestamp":1750434769937,"user_tz":-420,"elapsed":39,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["# Example with all optimizations\n","def create_optimized_pipeline(file_pattern, batch_size=32):\n","    # List files\n","# Membuat pipeline data menggunakan tf.data API\n","    dataset = tf.data.Dataset.list_files(file_pattern)\n","\n","    # Read files in parallel\n","    dataset = dataset.interleave(\n","# Membuat pipeline data menggunakan tf.data API\n","        tf.data.TextLineDataset,\n","# Membuat pipeline data menggunakan tf.data API\n","        cycle_length=tf.data.AUTOTUNE,\n","# Membuat pipeline data menggunakan tf.data API\n","        num_parallel_calls=tf.data.AUTOTUNE\n","    )\n","\n","    # Shuffle and batch\n","    dataset = dataset.shuffle(buffer_size=1000)\n","    dataset = dataset.batch(batch_size)\n","\n","    # Prefetch\n","# Membuat pipeline data menggunakan tf.data API\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","# Usage\n","optimized_dataset = create_optimized_pipeline(\"data/*.txt\")"]},{"cell_type":"markdown","metadata":{"id":"35820662"},"source":["## 5. **TFRecord** adalah format file biner efisien yang digunakan TensorFlow untuk menyimpan dan memproses data dalam skala besar. Format\n","\n","### 5.1 Creating **TFRecord** adalah format file biner efisien yang digunakan TensorFlow untuk menyimpan dan memproses data dalam skala besar. Files\n","Efficient binary format for large datasets:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3bfcfd79","executionInfo":{"status":"ok","timestamp":1750434772588,"user_tz":-420,"elapsed":4,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["def create_tfrecord_example(feature, label):\n","    feature_dict = {\n","        'feature': tf.train.Feature(float_list=tf.train.FloatList(value=feature)),\n","        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n","    }\n","    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n","    return example.SerializeToString()\n","\n","# Operasi terkait format file TFRecord\n","# Write TFRecord file\n","# Operasi terkait format file TFRecord\n","with tf.io.TFRecordWriter(\"data/sample.tfrecord\") as writer:\n","    for i in range(10):\n","        feature = np.random.rand(5).astype(np.float32)\n","        label = i % 2\n","        example = create_tfrecord_example(feature, label)\n","        writer.write(example)"]},{"cell_type":"markdown","metadata":{"id":"5bb9cd4e"},"source":["### 5.2 Reading **TFRecord** adalah format file biner efisien yang digunakan TensorFlow untuk menyimpan dan memproses data dalam skala besar. Files\n","Parse **TFRecord** adalah format file biner efisien yang digunakan TensorFlow untuk menyimpan dan memproses data dalam skala besar. data back into usable format:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0705b0d7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750434775310,"user_tz":-420,"elapsed":112,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"00898726-add3-40c4-a743-c8281c8735d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'feature': <tf.Tensor: shape=(5,), dtype=float32, numpy=\n","array([0.10959806, 0.5421599 , 0.45390922, 0.37752235, 0.5624958 ],\n","      dtype=float32)>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>}\n","{'feature': <tf.Tensor: shape=(5,), dtype=float32, numpy=\n","array([0.60083044, 0.40484416, 0.96339387, 0.1968151 , 0.10927536],\n","      dtype=float32)>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>}\n","{'feature': <tf.Tensor: shape=(5,), dtype=float32, numpy=\n","array([0.9359239 , 0.6607104 , 0.1501723 , 0.28888416, 0.64935267],\n","      dtype=float32)>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>}\n"]}],"source":["feature_description = {\n","    'feature': tf.io.FixedLenFeature([5], tf.float32),\n","    'label': tf.io.FixedLenFeature([], tf.int64),\n","}\n","\n","def parse_tfrecord(example_proto):\n","    return tf.io.parse_single_example(example_proto, feature_description)\n","\n","# Membuat pipeline data menggunakan tf.data API\n","dataset = tf.data.TFRecordDataset(\"data/sample.tfrecord\")\n","dataset = dataset.map(parse_tfrecord)\n","\n","for parsed_record in dataset.take(3):\n","    print(parsed_record)"]},{"cell_type":"markdown","metadata":{"id":"5d3c3562"},"source":["## 6. Keras Preprocessing Layers\n","\n","### 6.1 Built-in Preprocessing\n","New Keras layers for efficient preprocessing:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"eb6e7c91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750434779086,"user_tz":-420,"elapsed":97,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"0a61b43d-f44c-4737-c502-9c6eb68f7950"},"outputs":[{"output_type":"stream","name":"stdout","text":["Normalized mean: 4.053116e-08\n","Encoded categories: [2 1 3 2]\n"]}],"source":["# Mengimpor pustaka TensorFlow dan modul lain yang dibutuhkan\n","from tensorflow.keras.layers import Normalization, StringLookup\n","# Mengimpor pustaka TensorFlow dan modul lain yang dibutuhkan\n","import numpy as np\n","\n","# Numeric feature normalization\n","data = np.random.rand(100, 1) * 100\n","# Menerapkan preprocessing seperti normalisasi data\n","norm_layer = Normalization()\n","norm_layer.adapt(data)\n","normalized_data = norm_layer(data)\n","print(\"Normalized mean:\", np.mean(normalized_data))\n","\n","# Categorical feature encoding\n","categories = [\"cat\", \"dog\", \"bird\"]\n","lookup_layer = StringLookup(vocabulary=categories)\n","encoded = lookup_layer([\"dog\", \"cat\", \"bird\", \"dog\"])\n","print(\"Encoded categories:\", encoded.numpy())"]},{"cell_type":"markdown","metadata":{"id":"eb3b60c9"},"source":["## 7. Exercises\n","\n","1. Create a pipeline that reads CSV files, preprocesses numeric and categorical columns, and feeds to a model\n","2. Benchmark the performance difference between prefetching vs no prefetching\n","3. Implement a custom preprocessing layer for text data\n","4. Convert an image dataset to **TFRecord** adalah format file biner efisien yang digunakan TensorFlow untuk menyimpan dan memproses data dalam skala besar. format and create a loading pipeline"]},{"cell_type":"markdown","metadata":{"id":"b5e6cb8a"},"source":["## 8. Key Takeaways\n","\n","- The Data API provides flexible tools for efficient data loading\n","- Proper preprocessing is crucial for model performance\n","- **TFRecord** adalah format file biner efisien yang digunakan TensorFlow untuk menyimpan dan memproses data dalam skala besar. format is ideal for large datasets\n","- Keras preprocessing layers integrate seamlessly with models\n","- Optimization techniques can significantly improve training speed"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}