{"cells":[{"cell_type":"markdown","metadata":{"id":"77c5098b"},"source":["# **Chapter 18: From Theory to Implementation**"]},{"cell_type":"markdown","metadata":{"id":"cb717e4d"},"source":["## 1. Introduction to\n","### Pembelajaran Penguatan (Reinforcement Learning)\n","\n","**Key Components**:\n","- Agent: Learns to make decisions\n","- Environment: Where the **Agen** (agent) adalah entitas yang mengambil tindakan dalam suatu lingkungan. operates\n","- State (s): Current situation\n","- Action (a): Decision taken by **Agen** (agent) adalah entitas yang mengambil tindakan dalam suatu lingkungan.\n","- Reward (r): Feedback from **Lingkungan** (environment) merespons aksi agen dan memberikan feedback berupa **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar..\n","- Policy (π): Strategy for acting in states\n","\n","**The RL Loop**:\n","1. Agent observes state\n","2. Selects action based on **Policy** adalah strategi yang digunakan agen untuk memilih aksi berdasarkan keadaan saat ini.\n","3. Receives **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar. and new state\n","4. Updates **Policy** adalah strategi yang digunakan agen untuk memilih aksi berdasarkan keadaan saat ini."]},{"cell_type":"markdown","metadata":{"id":"112b1770"},"source":["## 2. Core RL Algorithms\n","\n","### 2.1 Q-Learning\n","Off-**Policy** adalah strategi yang digunakan agen untuk memilih aksi berdasarkan keadaan saat ini. TD learning that estimates the optimal Q-values:\n","\\[\n","Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]\n","\\]\n","\n","- $\\alpha$: Learning rate\n","- $\\gamma$: Discount factor\n","- $\\epsilon$: Exploration rate"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7a9d47e2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750437000224,"user_tz":-420,"elapsed":5634,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"65d430a2-e121-4f83-8279-92c0d84f7f20"},"outputs":[{"output_type":"stream","name":"stdout","text":["Q-table setelah pelatihan:\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["# Mengimpor pustaka yang diperlukan\n","import numpy as np\n","import gym\n","\n","# Inisialisasi environment\n","env = gym.make('FrozenLake-v1', is_slippery=False)\n","n_states = env.observation_space.n\n","n_actions = env.action_space.n\n","\n","# Inisialisasi Q-table\n","Q = np.zeros((n_states, n_actions))\n","\n","# Hyperparameter\n","alpha = 0.1\n","gamma = 0.99\n","epsilon = 0.1\n","n_episodes = 1000\n","\n","# Fungsi bantu agar reset kompatibel dengan semua versi gym\n","def safe_reset(env):\n","    result = env.reset()\n","    return result[0] if isinstance(result, tuple) else result\n","\n","# Fungsi bantu agar step kompatibel dengan semua versi gym\n","def safe_step(env, action):\n","    result = env.step(action)\n","    if len(result) == 5:\n","        next_state, reward, terminated, truncated, _ = result\n","        done = terminated or truncated\n","    elif len(result) == 4:\n","        next_state, reward, done, _ = result\n","    else:\n","        raise ValueError(\"Format hasil dari env.step() tidak dikenali.\")\n","    return next_state, reward, done\n","\n","\n","# Implementasi Q-learning\n","for episode in range(n_episodes):\n","    state = safe_reset(env)\n","    done = False\n","\n","    while not done:\n","        # Epsilon-greedy action selection\n","        if np.random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample()  # eksplorasi\n","        else:\n","            action = np.argmax(Q[state])  # eksploitasi\n","\n","        # Ambil aksi dan transisi\n","        next_state, reward, done = safe_step(env, action)\n","\n","        # Update Q-table\n","        Q[state, action] = Q[state, action] + alpha * (\n","            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n","        )\n","\n","        state = next_state\n","\n","# Output Q-table setelah pelatihan\n","print(\"Q-table setelah pelatihan:\")\n","print(Q)\n"]},{"cell_type":"markdown","metadata":{"id":"bd289dcd"},"source":["### 2.2 Deep Q-Networks (DQN)\n","Uses neural networks to approximate Q-values. Key innovations:\n","- Experience replay\n","- Target network\n","- Frame stacking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c719c64"},"outputs":[],"source":["# Mengimpor pustaka yang diperlukan seperti gym dan numpy\n","import tensorflow as tf\n","# Mengimpor pustaka yang diperlukan seperti gym dan numpy\n","from collections import deque\n","# Mengimpor pustaka yang diperlukan seperti gym dan numpy\n","import random\n","\n","class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=2000)\n","        self.gamma = 0.95    # discount rate\n","# Menentukan parameter epsilon untuk eksplorasi\n","        self.epsilon = 1.0   # exploration rate\n","# Menentukan parameter epsilon untuk eksplorasi\n","        self.epsilon_min = 0.01\n","# Menentukan parameter epsilon untuk eksplorasi\n","        self.epsilon_decay = 0.995\n","        self.learning_rate = 0.001\n","        self.model = self._build_model()\n","        self.target_model = self._build_model()\n","\n","    def _build_model(self):\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n","            tf.keras.layers.Dense(24, activation='relu'),\n","            tf.keras.layers.Dense(self.action_size, activation='linear')\n","        ])\n","        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","# Menentukan parameter epsilon untuk eksplorasi\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        act_values = self.model.predict(state)\n","        return np.argmax(act_values[0])\n","\n","    def replay(self, batch_size):\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            target = self.model.predict(state)\n","            if done:\n","                target[0][action] = reward\n","            else:\n","                t = self.target_model.predict(next_state)\n","                target[0][action] = reward + self.gamma * np.amax(t[0])\n","            self.model.fit(state, target, epochs=1, verbose=0)\n","        if self.epsilon > self.epsilon_min:\n","# Menentukan parameter epsilon untuk eksplorasi\n","            self.epsilon *= self.epsilon_decay\n","\n","    def update_target_model(self):\n","        self.target_model.set_weights(self.model.get_weights())"]},{"cell_type":"markdown","metadata":{"id":"fc21c919"},"source":["## 3. Policy Gradient Methods\n","\n","Directly optimize the **Policy** adalah strategi yang digunakan agen untuk memilih aksi berdasarkan keadaan saat ini. $\\pi_θ(a|s)$. The REINFORCE algorithm:\n","\\[\n","\\nabla_θ J(θ) = \\mathbb{E}[\\nabla_θ \\log \\pi_θ(a|s) G_t]\n","\\]\n","\n","Where $G_t$ is the return from time step $t$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d6321e0","executionInfo":{"status":"aborted","timestamp":1750437001182,"user_tz":-420,"elapsed":1,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["class PGAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = 0.95\n","        self.learning_rate = 0.001\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.model = self._build_model()\n","\n","    def _build_model(self):\n","        model = tf.keras.Sequential([\n","            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n","            tf.keras.layers.Dense(24, activation='relu'),\n","            tf.keras.layers.Dense(self.action_size, activation='softmax')\n","        ])\n","        model.compile(loss='categorical_crossentropy',\n","                    optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n","        return model\n","\n","    def remember(self, state, action, reward):\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.rewards.append(reward)\n","\n","    def act(self, state):\n","        state = np.reshape(state, [1, self.state_size])\n","        prob_weights = self.model.predict(state)\n","# Memilih aksi berdasarkan eksplorasi atau eksploitasi\n","        action = np.random.choice(self.action_size, p=prob_weights[0])\n","        return action\n","\n","    def train(self):\n","        discounted_rewards = []\n","        Gt = 0\n","        for reward in reversed(self.rewards):\n","            Gt = reward + self.gamma * Gt\n","            discounted_rewards.insert(0, Gt)\n","\n","        # Normalize rewards\n","        discounted_rewards = np.array(discounted_rewards)\n","        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / np.std(discounted_rewards)\n","\n","        # Convert actions to one-hot vectors\n","        actions = tf.keras.utils.to_categorical(self.actions, num_classes=self.action_size)\n","\n","        # Update policy\n","        self.model.train_on_batch(np.vstack(self.states), actions, sample_weight=discounted_rewards)\n","\n","        # Clear memory\n","        self.states, self.actions, self.rewards = [], [], []"]},{"cell_type":"markdown","metadata":{"id":"48a3f017"},"source":["## 4. Proximal Policy Optimization (PPO)\n","\n","Modern **Policy** adalah strategi yang digunakan agen untuk memilih aksi berdasarkan keadaan saat ini. gradient algorithm with:\n","- Clipped surrogate objective\n","- Multiple epochs per rollout\n","- Advantage normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70872560","executionInfo":{"status":"aborted","timestamp":1750437001196,"user_tz":-420,"elapsed":1,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["class PPOMemory:\n","    def __init__(self, batch_size):\n","        self.states = []\n","        self.actions = []\n","        self.probs = []\n","        self.vals = []\n","        self.rewards = []\n","        self.dones = []\n","        self.batch_size = batch_size\n","\n","    def store(self, state, action, prob, val, reward, done):\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.probs.append(prob)\n","        self.vals.append(val)\n","        self.rewards.append(reward)\n","        self.dones.append(done)\n","\n","    def clear(self):\n","        self.states = []\n","        self.actions = []\n","        self.probs = []\n","        self.vals = []\n","        self.rewards = []\n","        self.dones = []\n","\n","class PPOAgent:\n","    def __init__(self, state_dim, action_dim, action_std_init=0.6):\n","        self.policy = self._build_network(state_dim, action_dim)\n","        self.policy_old = self._build_network(state_dim, action_dim)\n","        self.memory = PPOMemory()\n","\n","    def _build_network(self, state_dim, action_dim):\n","        # Simplified network architecture\n","        inputs = tf.keras.layers.Input(shape=(state_dim,))\n","        x = tf.keras.layers.Dense(64, activation='tanh')(inputs)\n","        x = tf.keras.layers.Dense(64, activation='tanh')(x)\n","        mean = tf.keras.layers.Dense(action_dim, activation='tanh')(x)\n","        std = tf.keras.layers.Dense(action_dim, activation='softplus')(x)\n","        return tf.keras.Model(inputs, [mean, std])\n","\n","    def update(self):\n","        # PPO update logic would go here\n","        # Includes: advantages calculation, clipping, multiple epochs\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"e329639c"},"source":["## 5. Practical Considerations\n","\n","### 5.1 Reward Shaping\n","- Design **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar.s to guide learning\n","- Balance sparse vs dense **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar.s\n","- Avoid **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar. hacking\n","\n","### 5.2 Exploration Strategies\n","- ε-greedy\n","- Boltzmann exploration\n","- Noisy networks\n","- Intrinsic curiosity"]},{"cell_type":"markdown","metadata":{"id":"14fb90cf"},"source":["## 6. Exercises\n","\n","1. Implement Double DQN to reduce overestimation bias\n","2. Add prioritized experience replay to the DQN\n","3. Train a **Policy** adalah strategi yang digunakan agen untuk memilih aksi berdasarkan keadaan saat ini. gradient **Agen** (agent) adalah entitas yang mengambil tindakan dalam suatu lingkungan. on CartPole-v1\n","4. Compare performance of different exploration strategies\n","5. Visualize learned value functions and policies"]},{"cell_type":"markdown","metadata":{"id":"79fdb682"},"source":["## 6. Key Takeaways (Continued)\n","\n","- **Value-based methods** (**Q-learning** adalah algoritma pembelajaran penguatan yang mencoba belajar nilai maksimum aksi dalam suatu keadaan., DQN) learn optimal value functions\n","- **Policy-based methods** (REINFORCE, PPO) directly optimize policies\n","- **Model-based RL** learns **Lingkungan** (environment) merespons aksi agen dan memberikan feedback berupa **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar.. dynamics for planning\n","- **Exploration vs Exploitation** must be carefully balanced\n","- **Reward design** critically impacts learning success"]},{"cell_type":"markdown","metadata":{"id":"8c4c1874"},"source":["## 7. Implementing RL with TF-Agents\n","\n","TensorFlow Agents provides production-grade RL implementations:"]},{"cell_type":"code","source":["!pip uninstall -y keras keras-nightly keras-Preprocessing keras-vis\n","!pip install tensorflow==2.14\n","!pip install tf-agents==0.17.0\n","!pip install tensorflow-probability==0.23.0\n"],"metadata":{"id":"xV08BoGTKshz","executionInfo":{"status":"aborted","timestamp":1750437001197,"user_tz":-420,"elapsed":6824,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe574b62","executionInfo":{"status":"aborted","timestamp":1750437001198,"user_tz":-420,"elapsed":6823,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["import tensorflow as tf\n","from tf_agents.environments import suite_gym\n","from tf_agents.environments.tf_py_environment import TFPyEnvironment\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.networks import q_network\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","\n","\n","# ========================================================\n","# MEMBUAT ENVIRONMENT\n","# ========================================================\n","env_name = 'CartPole-v1'\n","\n","train_py_env = suite_gym.load(env_name)\n","eval_py_env = suite_gym.load(env_name)\n","\n","train_env = TFPyEnvironment(train_py_env)\n","eval_env = TFPyEnvironment(eval_py_env)\n","\n","# ========================================================\n","# MEMBUAT Q-NETWORK\n","# ========================================================\n","fc_layer_params = (100,)  # bisa ditambah layer lebih banyak\n","q_net = q_network.QNetwork(\n","    train_env.observation_spec(),\n","    train_env.action_spec(),\n","    fc_layer_params=fc_layer_params\n",")\n","\n","# ========================================================\n","# INISIALISASI AGENT\n","# ========================================================\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","train_step_counter = tf.Variable(0)\n","\n","agent = dqn_agent.DqnAgent(\n","    train_env.time_step_spec(),\n","    train_env.action_spec(),\n","    q_network=q_net,\n","    optimizer=optimizer,\n","    td_errors_loss_fn=common.element_wise_squared_loss,\n","    train_step_counter=train_step_counter\n",")\n","\n","agent.initialize()\n","\n","# ========================================================\n","# MEMBUAT REPLAY BUFFER\n","# ========================================================\n","replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","    data_spec=agent.collect_data_spec,\n","    batch_size=train_env.batch_size,\n","    max_length=100000\n",")\n","\n","print(\"✅ TF-Agents DQN Agent berhasil diinisialisasi.\")\n"]},{"cell_type":"markdown","metadata":{"id":"c1f5dd15"},"source":["## 8. Training Loop Example\n","\n","Complete training procedure with TF-Agents:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b3d9a2c","executionInfo":{"status":"aborted","timestamp":1750437001199,"user_tz":-420,"elapsed":6823,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["def train_agent(agent, env, replay_buffer, num_iterations=20000):\n","    # Create dataset from replay buffer\n","    dataset = replay_buffer.as_dataset(\n","        num_parallel_calls=3,\n","        sample_batch_size=64,\n","        num_steps=2).prefetch(3)\n","\n","    # Training loop\n","    for _ in range(num_iterations):\n","        # Collect experience\n","        time_step = env.current_time_step()\n","        action_step = agent.policy.action(time_step)\n","        next_time_step = env.step(action_step.action)\n","        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","        replay_buffer.add_batch(traj)\n","\n","        # Train on sampled experience\n","        experience, _ = next(iter(dataset))\n","        train_loss = agent.train(experience).loss\n","\n","        if _ % 1000 == 0:\n","            print(f'Iteration {_}: Loss = {train_loss}')\n","\n","    return agent"]},{"cell_type":"markdown","metadata":{"id":"ed7be479"},"source":["## 9. Advanced RL Techniques\n","\n","### 9.1 Imitation Learning\n","- Learn from expert demonstrations\n","- Behavioral cloning\n","- Dataset Aggregation (DAgger)\n","\n","### 9.2 Multi-Agent RL\n","- Competitive/cooperative **Lingkungan** (environment) merespons aksi agen dan memberikan feedback berupa **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar..s\n","- Markov Games framework\n","- Centralized training with decentralized execution\n","\n","### 9.3 Hierarchical RL\n","- Temporal abstraction with options\n","- Meta-controllers and sub-policies\n","- Feudal networks"]},{"cell_type":"markdown","metadata":{"id":"f0001079"},"source":["## 10. Debugging RL Systems\n","\n","Common issues and solutions:\n","\n","| Problem | Possible Causes | Solutions |\n","|---------|-----------------|-----------|\n","| No learning | Low exploration, bad **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar.s | Adjust ε, reshape **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar.s |\n","| Unstable training | High learning rate, small buffer | Reduce LR, increase buffer |\n","| Poor final performance | Limited capacity, local optima | Larger network, better exploration |\n","| High variance | Small batches, no target network | Increase batch size, add target net |"]},{"cell_type":"markdown","metadata":{"id":"73444441"},"source":["## 11. Final Summary\n","\n","- **Tabular methods** work well for small state spaces\n","- **Deep RL** scales to complex **Lingkungan** (environment) merespons aksi agen dan memberikan feedback berupa **Reward** adalah sinyal yang diterima agen berdasarkan aksi yang diambil, digunakan untuk belajar..s\n","- **Policy gradients** handle continuous actions naturally\n","- **TF-Agents** provides production-ready implementations\n","- **Careful experimentation** is key to successful RL applications"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}