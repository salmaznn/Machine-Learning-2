{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAjlLuWOIIRhDLHfvsSpGN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Chapter 5: Support Vector Machines (SVM)**\n","- Klasifikasi Linear SVM (hard margin dan soft margin)\n","- Kernel trick dan klasifikasi SVM nonlinier\n","- Regresi SVM\n","- Online SVM dan hinge loss\n"],"metadata":{"id":"NhaG_XlEEX4i"}},{"cell_type":"markdown","source":["# **Support Vector Machines**\n","\n","SVM merupakan model yang sangat efektif untuk klasifikasi, regresi, dan deteksi outlier. Model ini berusaha menemukan margin selebar mungkin (\"jalan\") yang memisahkan kelas dengan memaksimalkan jarak antara decision boundary dan data latih terdekat yang disebut **support vectors**.\n","\n"],"metadata":{"id":"gVY-KcQQEr5A"}},{"cell_type":"markdown","source":["# **Klasifikasi Linear SVM**\n","- **Hard margin**: Pemisahan ketat tanpa kesalahan, hanya berfungsi bila data benar-benar linier terpisah dan sensitif terhadap outlier.\n","- **Soft margin**: Mengizinkan pelanggaran margin terbatas, dikendalikan oleh hyperparameter \\( C \\), menyeimbangkan antara ukuran margin dan kesalahan klasifikasi untuk generalisasi lebih baik.\n","\n","### SVM Objective (Soft Margin)\n","\n","$$ \\min_{w,b,\\zeta} \\; \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{m} \\zeta_i \\quad \\text{dengan syarat:} \\quad y_i(w^T x_i + b) \\geq 1 - \\zeta_i,\\; \\zeta_i \\geq 0 $$\n","\n","### Fungsi prediksi SVM untuk instance baru \\( x \\):\n","\n","$$ f(x) = \\text{sign}(w^T x + b) $$\n","\n","Training mencari \\( w, b \\) untuk memaksimalkan margin sambil meminimalkan pelanggaran.\n","\n"],"metadata":{"id":"mK7q2PoJE1l3"}},{"cell_type":"code","source":["# Contoh klasifikasi Linear SVM dengan dataset iris\n","from sklearn.datasets import load_iris\n","from sklearn.svm import LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","import numpy as np\n","\n","iris = load_iris()\n","X = iris.data[:, (2, 3)]  # panjang dan lebar kelopak\n","y = (iris.target == 2).astype(np.float64)  # klasifikasi iris virginica\n","\n","svm_clf = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n","])\n","\n","svm_clf.fit(X, y)\n","print(svm_clf.predict([[5.5, 1.7]]))  # prediksi iris virginica"],"metadata":{"id":"Yu6eIviaFhkn","executionInfo":{"status":"ok","timestamp":1750433675369,"user_tz":-420,"elapsed":19,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"7c44fa9a-1f58-414b-fb31-b6879e77ef32","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.]\n"]}]},{"cell_type":"markdown","source":["# **Pentingnya Feature Scaling**\n","\n","SVM sangat sensitif terhadap skala fitur. Selalu lakukan standardisasi fitur (mean = 0, varian = 1) untuk margin dan performa yang lebih baik."],"metadata":{"id":"X6l5hZSOFFt_"}},{"cell_type":"markdown","source":["# **SVM Nonlinear dan Kernel Trick**\n","\n","Sebagian besar dataset tidak dapat dipisahkan secara linear. Dua pendekatan umum:\n","- Menambahkan fitur polinomial secara manual (mahal untuk derajat tinggi)\n","- Gunakan **kernel trick** untuk menghitung dot product dalam ruang berdimensi tinggi tanpa pemetaan eksplisit.\n","\n","### Kernel umum:\n","- Linear: $$ K(a, b) = a^T b $$\n","- Polynomial: $$ K(a, b) = (\\gamma a^T b + r)^d $$\n","- Gaussian RBF: $$ K(a, b) = \\exp(-\\gamma ||a - b||^2) $$\n","- Sigmoid: $$ K(a, b) = \\tanh(\\gamma a^T b + r) $$"],"metadata":{"id":"HEYC_LsVGik0"}},{"cell_type":"code","source":["# Contoh SVM nonlinier dengan kernel polinomial\n","from sklearn.datasets import make_moons\n","from sklearn.svm import SVC\n","\n","X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n","poly_kernel_svm_clf = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n","])\n","\n","poly_kernel_svm_clf.fit(X, y)\n","print(\"Jumlah support vectors:\", len(poly_kernel_svm_clf.named_steps[\"svm_clf\"].support_))"],"metadata":{"id":"Yn3hTvk1G0NX","executionInfo":{"status":"ok","timestamp":1750433675394,"user_tz":-420,"elapsed":18,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"ad9dbcb3-6bcd-4f7e-c21b-9b035539be75","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah support vectors: 19\n"]}]},{"cell_type":"markdown","source":["# **Gaussian RBF Kernel**\n","\n","Kernel Gaussian (RBF) memetakan data ke ruang berdimensi tak hingga, memungkinkan SVM mempelajari batas keputusan kompleks hanya dengan sedikit hyperparameter:\n","- $$\\gamma$$ seberapa besar pengaruh setiap titik\n","- \\( C \\): keseimbangan antara lebar margin dan pelanggaran"],"metadata":{"id":"xuCYMhsaG26X"}},{"cell_type":"code","source":["# Contoh SVM dengan kernel RBF\n","rbf_kernel_svm_clf = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=2, C=1))\n","])\n","\n","rbf_kernel_svm_clf.fit(X, y)\n","print(\"Jumlah support vectors:\", len(rbf_kernel_svm_clf.named_steps[\"svm_clf\"].support_))"],"metadata":{"id":"678j-R3MG2o0","executionInfo":{"status":"ok","timestamp":1750433675440,"user_tz":-420,"elapsed":39,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"af0a41a3-ab2b-460c-aecf-5a80f1c5fca4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Jumlah support vectors: 36\n"]}]},{"cell_type":"markdown","source":["# **Dual Problem dan Kernel Trick**\n","\n","Pelatihan SVM dapat dinyatakan sebagai masalah optimasi kuadratik ganda, fokus pada support vector dan dot product antar data.\n","\n","Dengan mengganti dot product dengan fungsi kernel, **kernel trick** memungkinkan pembelajaran dalam ruang fitur berdimensi tinggi tanpa transformasi eksplisit.\n","\n","Efeknya: komputasi lebih efisien dan klasifikasi nonlinier bisa dilakukan dengan baik."],"metadata":{"id":"_uAsDMXKHUZR"}},{"cell_type":"markdown","source":["# **SVM untuk Regresi (SVR)**\n","\n","SVM juga dapat digunakan untuk regresi dengan menyesuaikan batas kesalahan sebesar \\( \\varepsilon \\) di sekitar fungsi prediksi. Nilai di luar batas ini dikenakan penalti.\n","\n","Versi linier dan kernelisasi tersedia mirip dengan klasifikasi.\n"],"metadata":{"id":"ONfcUG1XHaCh"}},{"cell_type":"markdown","source":["# Contoh regresi menggunakan SVR\n","from sklearn.svm import LinearSVR\n","\n","X_reg = 2 * np.random.rand(100, 1)\n","y_reg = 4 + 3 * X_reg.ravel() + np.random.randn(100)\n","\n","svr_reg = LinearSVR(epsilon=0.5, C=1.0, max_iter=10000)\n","svr_reg.fit(X_reg, y_reg)\n","print(f\"Koefisien SVR: {svr_reg.coef_}\")"],"metadata":{"id":"2JxMWKT6Hdi2"}},{"cell_type":"markdown","source":["# **Online SVM dan Hinge Loss**\n","SVM dapat dilatih secara bertahap (online) menggunakan stochastic gradient descent dan fungsi kerugian hinge loss:\n","\n","$$ J(w, b) = \\frac{1}{2} ||w||^2 + C \\sum_i \\max(0, 1 - y_i(w^T x_i + b)) $$\n","\n","Fungsi loss ini menghukum prediksi yang salah atau terlalu dekat dengan margin.\n","\n","Pelatihan online cocok untuk dataset besar atau bersifat streaming.\n","\n","Untuk dataset sangat besar dan nonlinear, model seperti **neural network** sering lebih disarankan.\n"],"metadata":{"id":"8k8C_701Hf3o"}}]}