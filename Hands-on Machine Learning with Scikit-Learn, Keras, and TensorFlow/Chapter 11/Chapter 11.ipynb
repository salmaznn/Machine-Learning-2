{"cells":[{"cell_type":"markdown","metadata":{"id":"ab4f0462"},"source":["# **Chapter 11: Training Deep Neural Networks**"]},{"cell_type":"markdown","metadata":{"id":"81fd4610"},"source":["## 1. Introduction to Deep Neural Networks (DNNs)\n","- **Definition**: DNNs are neural networks with multiple hidden layers that can learn complex representations of data.\n","- **Applications**: Used in various fields such as image recognition, natural language processing, and more.\n","- **Challenges**: Training DNNs can be difficult due to issues like vanishing/exploding gradients, slow convergence, and overfitting."]},{"cell_type":"markdown","metadata":{"id":"f5ee0d91"},"source":["## 2. Common Problems in Training DNNs\n","### 2.1 Vanishing and Exploding Gradients\n","- **Vanishing Gradients**: Gradients become very small, making it hard for the network to learn.\n","- **Exploding Gradients**: Gradients become excessively large, causing the model to diverge.\n","- **Solution**: Use activation functions like ReLU, which help mitigate these issues."]},{"cell_type":"markdown","metadata":{"id":"a260be21"},"source":["### 2.2 Overfitting\n","- **Definition**: When a model learns noise in the training data instead of the underlying pattern.\n","- **Symptoms**: High accuracy on training data but poor performance on validation/test data.\n","- **Solutions**: Use techniques like dropout, L2 regularization, and early stopping."]},{"cell_type":"markdown","metadata":{"id":"748b2d7a"},"source":["## 3. Techniques to Train DNNs\n","### 3.1 Weight Initialization\n","- **Importance**: Proper initialization can prevent vanishing/exploding gradients.\n","- **Methods**:\n","  - **Xavier Initialization**: Suitable for sigmoid/tanh activations.\n","  - **He Initialization**: Recommended for ReLU activations."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8761eb94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750434549279,"user_tz":-420,"elapsed":129,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"22c61b66-80ad-42a0-ee5f-2b16f534ab6c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["from tensorflow import keras\n","\n","# Contoh jumlah fitur input\n","input_dim = 10  # ganti sesuai jumlah kolom di dataset Anda\n","\n","# Membangun arsitektur model jaringan saraf dengan inisialisasi He\n","model = keras.Sequential([\n","    keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', input_shape=(input_dim,)),\n","    keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Menyusun model dengan fungsi loss dan optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"markdown","metadata":{"id":"48a84bfb"},"source":["### 3.2 **Batch Normalization** membantu stabilisasi\n","- **Definition**: A technique to normalize the inputs of each layer to improve training speed and stability.\n","- **Benefits**: Reduces internal covariate shift, allows for higher learning rates, and acts as a regularizer."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"67b861d1","executionInfo":{"status":"ok","timestamp":1750434552444,"user_tz":-420,"elapsed":42,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["# Example of Batch Normalization in Keras\n","# Membangun arsitektur model jaringan saraf\n","model = keras.Sequential([\n","    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Dense(32, activation='relu'),\n","    keras.layers.BatchNormalization(),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","# Menyusun model dengan fungsi loss dan optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"f1fe85c1"},"source":["### 3.3 Optimizers\n","- **Gradient Descent Variants**: Different optimizers can significantly affect training speed and convergence.\n","- **Common Optimizers**:\n","  - **SGD (Stochastic Gradient Descent)**: Basic optimizer.\n","  - **Adam**: Adaptive learning rate optimizer that combines the benefits of AdaGrad and RMSProp."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"a8b91ef9","executionInfo":{"status":"ok","timestamp":1750434554495,"user_tz":-420,"elapsed":12,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["# Example of using Adam optimizer in Keras\n","# Menyusun model dengan fungsi loss dan optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"262e8b28"},"source":["## 4. Regularization Techniques\n","### 4.1 Dropout\n","- **Definition**: A technique where randomly selected neurons are ignored during training to prevent overfitting.\n","- **Implementation**: Specify a dropout rate (e.g., 0.5 for 50% of neurons to be dropped)."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"e68f1f75","executionInfo":{"status":"ok","timestamp":1750434556265,"user_tz":-420,"elapsed":51,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["# Example of Dropout in Keras\n","# Membangun arsitektur model jaringan saraf\n","model = keras.Sequential([\n","    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(32, activation='relu'),\n","    keras.layers.Dropout(0.5),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","# Menyusun model dengan fungsi loss dan optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"c82ed440"},"source":["### 4.2 L2 Regularization\n","- **Definition**: Adds a penalty equal to the square of the magnitude of coefficients to the loss function.\n","- **Purpose**: Helps to prevent overfitting by discouraging overly complex models."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"69c4b6ed","executionInfo":{"status":"ok","timestamp":1750434558255,"user_tz":-420,"elapsed":34,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}}},"outputs":[],"source":["# Example of L2 Regularization in Keras\n","# Mengimpor pustaka yang dibutuhkan\n","from tensorflow.keras import regularizers\n","\n","# Membangun arsitektur model jaringan saraf\n","model = keras.Sequential([\n","    keras.layers.Dense(64, activation='relu', input_shape=(input_dim,), kernel_regularizer=regularizers.l2(0.01)),\n","    keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","# Menyusun model dengan fungsi loss dan optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"ad99fc9c"},"source":["## 5. Conclusion\n","- Training deep neural networks involves careful consideration of architecture, initialization, optimization, and regularization techniques.\n","- Understanding these concepts is crucial for building effective models that generalize well to unseen data."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}