{"cells":[{"cell_type":"markdown","metadata":{"id":"b6191da5"},"source":["# **Chapter 16: NLP Implementation Guide**"]},{"cell_type":"markdown","metadata":{"id":"dbfa8c8d"},"source":["## 1. Introduction to NLP\n","\n","Natural Language Processing (NLP) enables machines to understand, interpret, and generate human language. Key applications include:\n","- Machine translation\n","- Sentiment analysis\n","- Text generation\n","- Question answering\n","\n","**Core Challenges**:\n","- Variable-length sequences\n","- Contextual meaning\n","- Ambiguity and polysemy"]},{"cell_type":"markdown","metadata":{"id":"235a6fd1"},"source":["## 2. Text Preprocessing\n","\n","### 2.1 Tokenization\n","Convert text to numerical representations:\n","- Word-level\n","- Character-level\n","- Subword (BPE, WordPiece)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"23e0e11e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750435393010,"user_tz":-420,"elapsed":7739,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"6ddbaf64-000c-4e75-f2f0-817af294baee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word index: {'<OOV>': 1, 'natural': 2, 'language': 3, 'processing': 4, 'is': 5, 'fascinating': 6, 'deep': 7, 'learning': 8, 'models': 9, 'can': 10, 'understand': 11, 'text': 12}\n","Padded sequences:\n"," [[ 2  3  4  5  6  0]\n"," [ 7  8  9 10 11 12]]\n"]}],"source":["# Mengimpor pustaka penting untuk NLP dan RNN\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","# Mengimpor pustaka penting untuk NLP dan RNN\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample text data\n","texts = [\"Natural language processing is fascinating\",\n","         \"Deep learning models can understand text\"]\n","\n","# Melatih model dengan dataset NLP\n","# Create and fit tokenizer\n","tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n","# Melatih model dengan dataset NLP\n","tokenizer.fit_on_texts(texts)\n","\n","# Convert text to sequences\n","sequences = tokenizer.texts_to_sequences(texts)\n","padded = pad_sequences(sequences, padding='post')\n","\n","print(\"Word index:\", tokenizer.word_index)\n","print(\"Padded sequences:\\n\", padded)"]},{"cell_type":"markdown","metadata":{"id":"023fb9da"},"source":["### 2.2 Embedding Layers\n","Map tokens to dense vectors capturing semantic meaning:\n","- Pretrained embeddings (GloVe, Word2Vec)\n","- Trainable embeddings"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2feca33c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750435393056,"user_tz":-420,"elapsed":50,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"4a1208bd-bea2-4278-c9a4-4977960a26c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedded shape: (32, 10, 64)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]}],"source":["# Mengimpor pustaka penting untuk NLP dan RNN\n","from tensorflow.keras.layers import Embedding\n","\n","# Create embedding layer\n","# Layer embedding: mengubah kata menjadi representasi vektor\n","embedding_layer = Embedding(\n","    input_dim=100,  # Vocabulary size\n","# Layer embedding: mengubah kata menjadi representasi vektor\n","    output_dim=64,  # Embedding dimension\n","    input_length=10  # Max sequence length\n",")\n","\n","# Example usage\n","# Mengimpor pustaka penting untuk NLP dan RNN\n","import numpy as np\n","sample_input = np.random.randint(0, 100, size=(32, 10))\n","embedded = embedding_layer(sample_input)\n","print(\"Embedded shape:\", embedded.shape)"]},{"cell_type":"markdown","metadata":{"id":"a6faac71"},"source":["## 3. Sequence Models for NLP\n","\n","### 3.1 RNN-based Models\n","- Process text sequentially\n","- Capture temporal dependencies\n","- Variants: LSTM, GRU (address vanishing gradient problem)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"df2dd479","colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"status":"ok","timestamp":1750435393152,"user_tz":-420,"elapsed":88,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"94f34af4-61a1-402a-9656-466986f6cf6c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["# Mengimpor pustaka penting untuk NLP dan RNN\n","from tensorflow.keras.models import Sequential\n","# Mengimpor pustaka penting untuk NLP dan RNN\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","# LSTM model for text classification\n","model = Sequential([\n","# Layer embedding: mengubah kata menjadi representasi vektor\n","    Embedding(10000, 128, input_length=100),\n","    LSTM(64, return_sequences=True),\n","    LSTM(32),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Menyusun model dengan optimizer dan loss function\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"de77177a"},"source":["### 3.2 CNN for Text\n","- Process text as 1D signals\n","- Can capture local patterns effectively\n","- Often combined with RNNs"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"98aa9953","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1750435393509,"user_tz":-420,"elapsed":356,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"683dfe0f-8a72-436d-e332-644266a3ccf3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["# Mengimpor pustaka penting untuk NLP dan RNN\n","from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n","\n","# TextCNN model\n","textcnn = Sequential([\n","# Layer embedding: mengubah kata menjadi representasi vektor\n","    Embedding(10000, 128, input_length=100),\n","    Conv1D(128, 5, activation='relu'),\n","    GlobalMaxPooling1D(),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Menyusun model dengan optimizer dan loss function\n","textcnn.compile(loss='binary_crossentropy', optimizer='adam')\n","textcnn.summary()"]},{"cell_type":"markdown","metadata":{"id":"e9315e2a"},"source":["## 4. Attention Mechanisms\n","\n","### 4.1 Basic Attention\n","- Allows models to focus on relevant parts of input\n","- Computes context vectors as weighted sums\n","- Particularly useful for long sequences"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"a3370035","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750435629017,"user_tz":-420,"elapsed":526,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"c7f11bfc-d76c-445f-997b-03c258f0b351"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context vector shape: (1, 20)\n"]}],"source":["import tensorflow as tf\n","\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super().__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        # Hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","        # Score shape == (batch_size, max_length, 1)\n","        score = self.V(tf.nn.tanh(\n","            self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","        # attention_weights shape == (batch_size, max_length, 1)\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        # context_vector shape == (batch_size, hidden_size)\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","        return context_vector, attention_weights\n","\n","# Example usage\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","attention_layer = BahdanauAttention(10)\n","query = tf.random.normal((1, 20))  # Decoder hidden state\n","values = tf.random.normal((1, 50, 20))  # Encoder outputs\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","context_vector, attention_weights = attention_layer(query, values)\n","print(\"Context vector shape:\", context_vector.shape)"]},{"cell_type":"markdown","metadata":{"id":"eb955a2f"},"source":["## 5. Transformer Architecture\n","\n","### 5.1 Key Components\n","- Self-attention mechanism\n","- Multi-head attention\n","- Positional encoding\n","- Layer normalization\n","\n","### 5.2 Implementation Example"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"42430623","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750435633243,"user_tz":-420,"elapsed":187,"user":{"displayName":"Salma Zanuba","userId":"01971269641838452130"}},"outputId":"5f08af25-1d3c-4cd6-a48d-e21ee2870589"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 10, 32)\n"]}],"source":["class TransformerBlock(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","# Menambahkan mekanisme attention untuk fokus pada bagian input yang penting\n","        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = tf.keras.Sequential([\n","            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n","            tf.keras.layers.Dense(embed_dim),\n","        ])\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","# Example usage\n","transformer_block = TransformerBlock(embed_dim=32, num_heads=2, ff_dim=64)\n","print(transformer_block(tf.random.uniform((1, 10, 32)), training=False).shape)"]},{"cell_type":"markdown","metadata":{"id":"2e5fe271"},"source":["## 6. Practical Applications\n","\n","### 6.1 Sentiment Analysis\n","- Classify text sentiment (positive/negative)\n","- Can use RNNs, CNNs, or Transformers\n","\n","### 6.2 Neural Machine Translation\n","- Encoder-decoder architecture\n","- Attention improves performance"]},{"cell_type":"markdown","metadata":{"id":"c40eaad2"},"source":["## 7. Exercises\n","\n","1. Implement a character-level RNN for text generation\n","2. Compare word vs. subword tokenization\n","3. Add attention to a sequence-to-sequence model\n","4. Fine-tune a pretrained transformer model\n","5. Visualize attention weights for sample inputs"]},{"cell_type":"markdown","metadata":{"id":"9619982c"},"source":["## 8. Key Takeaways\n","\n","- NLP requires specialized preprocessing and tokenization\n","- RNNs and CNNs can effectively process text sequences\n","- **Mekanisme Attention** memungkinkan model untuk memperhatikan bagian penting dari input saat membuat prediksi, meningkatkan konteks dan akurasi. enable models to focus on relevant context\n","- Transformers have become the state-of-the-art for many tasks\n","- Pretrained models (BERT, GPT) provide powerful starting points"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}